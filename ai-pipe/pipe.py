from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import torch
import requests
from transformers import BitsAndBytesConfig
from transformers import pipeline

class Lava:
    def __init__(self):
        self.model = 0
        self.model_id = 0
        self.processor = 0
        self.device = 0
        self.image = 0

    def load_model(self):
        self.model_id = "llava-hf/llava-1.5-7b-hf"
        self.processor = AutoProcessor.from_pretrained(self.model_id)
        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
        self.model = AutoModelForVision2Seq.from_pretrained(self.model_id, quantization_config=quantization_config)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

    def load_image(self, path):
        self.image = Image.open(path).convert("RGB")

    def inference(self):
        inputs = self.processor(text="USER: <image>\nWho? What? Where?\nASSISTANT:", images=self.image, return_tensors="pt").to(self.device) # Move inputs to GPU
        print("Number of pixel values (image features) generated by processor:", inputs['pixel_values'].shape[1])
        try:
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=512)
                return self.processor.decode(outputs[0], skip_special_tokens=True)
        except ValueError as e:
            print("Forward pass error:", e)
            return e
        except torch.cuda.OutOfMemoryError as e:
            print("CUDA Out of Memory Error:", e)
            return e
        except RuntimeError as e:
            print("Runtime Error during generation:", e)
            return e
